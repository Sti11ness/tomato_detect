{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0bf8f1cc",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                      Version\n",
      "---------------------------- ------------\n",
      "absl-py                      1.4.0\n",
      "anyio                        3.7.1\n",
      "argon2-cffi                  21.3.0\n",
      "argon2-cffi-bindings         21.2.0\n",
      "arrow                        1.2.3\n",
      "asttokens                    2.2.1\n",
      "astunparse                   1.6.3\n",
      "attrs                        23.1.0\n",
      "backcall                     0.2.0\n",
      "beautifulsoup4               4.12.2\n",
      "bleach                       6.0.0\n",
      "cachetools                   5.3.1\n",
      "certifi                      2022.12.7\n",
      "cffi                         1.15.1\n",
      "charset-normalizer           2.1.1\n",
      "colorama                     0.4.6\n",
      "comm                         0.1.3\n",
      "contourpy                    1.1.0\n",
      "cycler                       0.11.0\n",
      "debugpy                      1.6.7\n",
      "decorator                    5.1.1\n",
      "defusedxml                   0.7.1\n",
      "distlib                      0.3.6\n",
      "executing                    1.2.0\n",
      "fastjsonschema               2.17.1\n",
      "filelock                     3.12.2\n",
      "flatbuffers                  23.5.26\n",
      "fonttools                    4.40.0\n",
      "fqdn                         1.5.1\n",
      "gast                         0.4.0\n",
      "google-auth                  2.21.0\n",
      "google-auth-oauthlib         1.0.0\n",
      "google-pasta                 0.2.0\n",
      "grpcio                       1.56.0\n",
      "h5py                         3.9.0\n",
      "idna                         3.4\n",
      "ipykernel                    6.24.0\n",
      "ipython                      8.14.0\n",
      "ipython-genutils             0.2.0\n",
      "ipywidgets                   8.0.7\n",
      "isoduration                  20.11.0\n",
      "jax                          0.4.13\n",
      "jedi                         0.18.2\n",
      "Jinja2                       3.1.2\n",
      "jsonpointer                  2.4\n",
      "jsonschema                   4.18.0\n",
      "jsonschema-specifications    2023.6.1\n",
      "jupyter                      1.0.0\n",
      "jupyter_client               8.3.0\n",
      "jupyter-console              6.6.3\n",
      "jupyter_core                 5.3.1\n",
      "jupyter-events               0.6.3\n",
      "jupyter_server               2.7.0\n",
      "jupyter_server_terminals     0.4.4\n",
      "jupyterlab-pygments          0.2.2\n",
      "jupyterlab-widgets           3.0.8\n",
      "keras                        2.12.0\n",
      "kiwisolver                   1.4.4\n",
      "libclang                     16.0.0\n",
      "Markdown                     3.4.3\n",
      "MarkupSafe                   2.1.3\n",
      "matplotlib                   3.7.2\n",
      "matplotlib-inline            0.1.6\n",
      "mistune                      3.0.1\n",
      "ml-dtypes                    0.2.0\n",
      "mpmath                       1.2.1\n",
      "nbclassic                    1.0.0\n",
      "nbclient                     0.8.0\n",
      "nbconvert                    7.6.0\n",
      "nbformat                     5.9.0\n",
      "nest-asyncio                 1.5.6\n",
      "networkx                     3.0\n",
      "notebook                     6.5.4\n",
      "notebook_shim                0.2.3\n",
      "numpy                        1.23.5\n",
      "oauthlib                     3.2.2\n",
      "opencv-python                4.8.0.74\n",
      "opt-einsum                   3.3.0\n",
      "overrides                    7.3.1\n",
      "packaging                    23.1\n",
      "pandas                       2.0.3\n",
      "pandocfilters                1.5.0\n",
      "parso                        0.8.3\n",
      "pickleshare                  0.7.5\n",
      "Pillow                       9.3.0\n",
      "pip                          23.2.1\n",
      "platformdirs                 3.8.1\n",
      "prometheus-client            0.17.0\n",
      "prompt-toolkit               3.0.39\n",
      "protobuf                     4.23.4\n",
      "psutil                       5.9.5\n",
      "pure-eval                    0.2.2\n",
      "pyasn1                       0.5.0\n",
      "pyasn1-modules               0.3.0\n",
      "pycocotools                  2.0.6\n",
      "pycparser                    2.21\n",
      "Pygments                     2.15.1\n",
      "pyparsing                    3.0.9\n",
      "python-dateutil              2.8.2\n",
      "python-json-logger           2.0.7\n",
      "pytz                         2023.3\n",
      "pywin32                      306\n",
      "pywinpty                     2.0.10\n",
      "PyYAML                       6.0\n",
      "pyzmq                        25.1.0\n",
      "qtconsole                    5.4.3\n",
      "QtPy                         2.3.1\n",
      "referencing                  0.29.1\n",
      "requests                     2.28.1\n",
      "requests-oauthlib            1.3.1\n",
      "rfc3339-validator            0.1.4\n",
      "rfc3986-validator            0.1.1\n",
      "rpds-py                      0.8.8\n",
      "rsa                          4.9\n",
      "scipy                        1.11.1\n",
      "seaborn                      0.12.2\n",
      "Send2Trash                   1.8.2\n",
      "setuptools                   65.5.0\n",
      "six                          1.16.0\n",
      "sniffio                      1.3.0\n",
      "soupsieve                    2.4.1\n",
      "stack-data                   0.6.2\n",
      "sympy                        1.11.1\n",
      "tensorboard                  2.12.3\n",
      "tensorboard-data-server      0.7.1\n",
      "tensorflow                   2.12.0\n",
      "tensorflow-estimator         2.12.0\n",
      "tensorflow-hub               0.14.0\n",
      "tensorflow-intel             2.12.0\n",
      "tensorflow-io-gcs-filesystem 0.31.0\n",
      "termcolor                    2.3.0\n",
      "terminado                    0.17.1\n",
      "tinycss2                     1.2.1\n",
      "torch                        2.0.1+cu118\n",
      "torchaudio                   2.0.2+cu118\n",
      "torchvision                  0.15.2+cu118\n",
      "tornado                      6.3.2\n",
      "tqdm                         4.65.0\n",
      "traitlets                    5.9.0\n",
      "typing_extensions            4.4.0\n",
      "tzdata                       2023.3\n",
      "ultralytics                  8.0.130\n",
      "uri-template                 1.3.0\n",
      "urllib3                      1.26.13\n",
      "virtualenv                   20.23.1\n",
      "virtualenvwrapper-win        1.2.7\n",
      "wcwidth                      0.2.6\n",
      "webcolors                    1.13\n",
      "webencodings                 0.5.1\n",
      "websocket-client             1.6.1\n",
      "Werkzeug                     2.3.6\n",
      "wheel                        0.40.0\n",
      "widgetsnbextension           4.0.8\n",
      "wrapt                        1.14.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "128fe150",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'LoadImagesAndLabels' from 'torchvision' (C:\\Users\\godcl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mPIL\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Image\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorchvision\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoadImagesAndLabels\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Указываем путь до датасета\u001b[39;00m\n\u001b[0;32m      9\u001b[0m dataset_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mgodcl\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mРабочий стол\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mTomatoPlantfactoryDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mImportError\u001b[0m: cannot import name 'LoadImagesAndLabels' from 'torchvision' (C:\\Users\\godcl\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\__init__.py)"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "from torch.utils.data import DataLoader, Dataset, dataloader, distributed\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from torchvision import LoadImagesAndLabels\n",
    "\n",
    "# Указываем путь до датасета\n",
    "dataset_path = \"C:\\\\Users\\\\godcl\\\\Рабочий стол\\\\TomatoPlantfactoryDataset\"\n",
    "\n",
    "# Загружаем датасет\n",
    "dataset = LoadImagesAndLabels(path=dataset_path, img_size=640, batch_size=16)\n",
    "\n",
    "# Создаем модель YOLOv5\n",
    "model = Model(cfg='yolov5s.yaml', ch=3, nc=1)  # nc - количество классов\n",
    "\n",
    "# Обучаем модель\n",
    "model.fit(dataset)\n",
    "\n",
    "# После обучения можно использовать модель для детекции объектов на новых изображениях\n",
    "img = torch.zeros((1, 3, 640, 640))  # замените эту строку на реальное изображение\n",
    "predictions = model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b718ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install ultralytic\n",
    "# pip install torch\n",
    "# pip install tensorflow\n",
    "# pip install pycocotools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "732e4fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import numpy as np\n",
    "from pycocotools.coco import COCO\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Путь к директории с данными COCO\n",
    "data_dir = 'C:\\Users\\godcl\\jupyter\\tomatoOD'\n",
    "batch_size = 32  \n",
    "num_epochs = 20\n",
    "\n",
    "# Путь к файлу с описанием train.json\n",
    "train_json_path = os.path.join(data_dir, 'annotations', 'tomatOD_train.json')\n",
    "\n",
    "# Загружаем аннотации и описания изображений для train.json\n",
    "with open(train_json_path, 'r') as f:\n",
    "    train_data = json.load(f)\n",
    "\n",
    "# Генерируем данные для обучения с использованием ImageDataGenerator\n",
    "train_data_gen = ImageDataGenerator(\n",
    "    rescale=1.0/255,  # Нормализация значений пикселей до диапазона [0, 1]\n",
    "    rotation_range=20,  # Случайные повороты изображений на угол до 20 градусов\n",
    "    width_shift_range=0.2,  # Случайное смещение по горизонтали до 20% от ширины изображения\n",
    "    height_shift_range=0.2,  # Случайное смещение по вертикали до 20% от высоты изображения\n",
    "    horizontal_flip=True  # Случайное отражение по горизонтали\n",
    ")\n",
    "\n",
    "train_images_list = [image['file_name'] for image in train_data['images']]\n",
    "train_labels = [class_labels_to_ids[train_data['categories'][ann['category_id'] - 1]['name']]\n",
    "                for ann in train_data['annotations']]\n",
    "\n",
    "# Создание генератора для тестовых данных\n",
    "test_data_gen = ImageDataGenerator(rescale=1. / 255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")    \n",
    "    \n",
    "# Подготовка данных для обучения\n",
    "train_images_dir = os.path.join(data_dir, 'train')\n",
    "image_height, image_width = 224, 224  # Размер изображения, который поддерживает EfficientNetB0\n",
    "num_classes = len(train_data['categories'])\n",
    "class_labels = [category['name'] for category in train_data['categories']]\n",
    "class_labels_to_ids = {label: idx for idx, label in enumerate(class_labels)}\n",
    "\n",
    "# Создание модели EfficientNetB0\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=(image_height, image_width, 3))\n",
    "\n",
    "# Замораживаем базовую модель для предотвращения обновления весов\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(1024, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# Компилируем модель\n",
    "model.compile(optimizer=Adam(lr=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "\n",
    "Запуск обучения модели\n",
    "model.fit(\n",
    "    train_data_gen.flow_from_directory(\n",
    "        train_images_dir,\n",
    "        target_size=(image_height, image_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        classes=train_images_list,  # Классы изображений\n",
    "        y=train_labels,  # Метки классов для изображений\n",
    "        shuffle=False  # Не перемешиваем изображения, чтобы метки соответствовали правильному порядку\n",
    "    ),\n",
    "    steps_per_epoch=len(train_labels) // batch_size,\n",
    "    epochs=num_epochs\n",
    ")\n",
    "# model.save('effnet.h5')\n",
    "# # Оценка точности модели на тестовом наборе\n",
    "# accuracy = model.evaluate(test_generator)[1]\n",
    "# print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de0534ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(\n",
    "    train_data_gen.flow_from_directory(\n",
    "        train_images_dir,\n",
    "        target_size=(image_height, image_width),\n",
    "        batch_size=batch_size,\n",
    "        class_mode='categorical',\n",
    "        classes=train_images_list,  # Классы изображений\n",
    "        y=train_labels,  # Метки классов для изображений\n",
    "        shuffle=False  # Не перемешиваем изображения, чтобы метки соответствовали правильному порядку\n",
    "    ),\n",
    "    steps_per_epoch=len(train_labels) // batch_size,\n",
    "    epochs=num_epochs\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29356753",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('effnet.h5')\n",
    "# Оценка точности модели на тестовом наборе\n",
    "accuracy = model.evaluate(test_generator)[1]\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d446b4a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import tensorflow as tf \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cecd9de",
   "metadata": {},
   "source": [
    "# Обучаем yolov8n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8277dd6",
   "metadata": {},
   "outputs": [
    {
     "ename": "error",
     "evalue": "OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:210: error: (-212:Parsing error) Failed to open NetParameter file: yolov8.cfg in function 'cv::dnn::dnn4_v20230620::readNetFromDarknet'\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Загрузка YOLO\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mcv2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadNet\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolov8.weights\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43myolov8.cfg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m net\u001b[38;5;241m.\u001b[39mgetLayerNames()\n\u001b[0;32m      8\u001b[0m output_layers \u001b[38;5;241m=\u001b[39m [layer_names[i[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m net\u001b[38;5;241m.\u001b[39mgetUnconnectedOutLayers()]\n",
      "\u001b[1;31merror\u001b[0m: OpenCV(4.8.0) D:\\a\\opencv-python\\opencv-python\\opencv\\modules\\dnn\\src\\darknet\\darknet_importer.cpp:210: error: (-212:Parsing error) Failed to open NetParameter file: yolov8.cfg in function 'cv::dnn::dnn4_v20230620::readNetFromDarknet'\n"
     ]
    }
   ],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ad3544",
   "metadata": {},
   "source": [
    "# Обучаем effnetb3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cc304f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import Sequence\n",
    "from keras.applications.resnet_v2 import preprocess_input\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "# Пути к данным и аннотациям\n",
    "data_dir = 'C:\\Users\\godcl\\jupyter\\data'\n",
    "train_data_dir = os.path.join(data_dir, 'train')\n",
    "test_data_dir = os.path.join(data_dir, 'test')\n",
    "#valid_data_dir = os.path.join(data_dir, 'valid')\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1. / 255,\n",
    "    rotation_range=10,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.1,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=False\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    train_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "# Создание генератора для тестовых данных\n",
    "test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    test_data_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "\n",
    "# Загрузка предварительно обученной модели ResNet101 без последнего слоя\n",
    "base_model = EfficientNetB0(\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    "    input_tensor=None,\n",
    "    input_shape=(224, 224, 3)\n",
    ")\n",
    "# Заморозка весов предварительно обученной модели\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "# Добавление своих слоев поверх предварительно обученной модели\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "predictions = Dense(train_generator.num_classes, activation='softmax')(x)\n",
    "# Создание модели для обучения\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Компиляция и обучение модели\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "model.fit(train_generator, epochs=8)\n",
    "\n",
    "#Размораживаем 20 слоев и дообучаем\n",
    "for layer in model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(train_generator, epochs=8, callbacks=[early_stopping])\n",
    "\n",
    "# Сохранение модели\n",
    "\n",
    "# Оценка точности модели на тестовом наборе\n",
    "accuracy = model.evaluate(test_generator)[1]\n",
    "print(f\"Accuracy: {accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "034198bd",
   "metadata": {},
   "source": [
    "# Загружаем модели и поочередно вырезаем боксы и потмо классифицируем их"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25abeaab",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import torch\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import tensorflow as tf \n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "# Load a model\n",
    "model = YOLO('1.pt')  # load a pretrained model (recommended for training)\n",
    "\n",
    "#Train the model\n",
    "#model.train(data='C:\\\\Users\\\\godcl\\\\jupyter\\\\ds2\\\\data.yaml', epochs=10, imgsz=416)\n",
    "\n",
    "img = Image.open('C:\\\\Users\\\\godcl\\\\Downloads\\\\tomato.jpg')\n",
    "results = model.predict(source=img, project='C:\\\\Users\\\\godcl\\\\', name='yoloresults',save = True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e0a3f0",
   "metadata": {},
   "source": [
    "# Обучаем effnetb3(ds = 55 test, 222 train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9577ef9f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение\n",
      "Epoch 1/10\n",
      "6/6 [==============================] - 15s 2s/step - loss: 5.0071 - accuracy: 0.6979 - val_loss: 4.5409 - val_accuracy: 0.7812\n",
      "Epoch 2/10\n",
      "6/6 [==============================] - 9s 2s/step - loss: 4.2590 - accuracy: 0.8105 - val_loss: 3.9443 - val_accuracy: 0.7812\n",
      "Epoch 3/10\n",
      "6/6 [==============================] - 9s 2s/step - loss: 3.7565 - accuracy: 0.7684 - val_loss: 3.4927 - val_accuracy: 0.7812\n",
      "Epoch 4/10\n",
      "6/6 [==============================] - 9s 2s/step - loss: 3.1415 - accuracy: 0.8421 - val_loss: 3.0402 - val_accuracy: 0.7812\n",
      "Epoch 5/10\n",
      "6/6 [==============================] - 9s 2s/step - loss: 2.8568 - accuracy: 0.7947 - val_loss: 2.6903 - val_accuracy: 0.7812\n",
      "Epoch 6/10\n",
      "6/6 [==============================] - 12s 2s/step - loss: 2.4570 - accuracy: 0.8105 - val_loss: 2.3851 - val_accuracy: 0.7812\n",
      "Epoch 7/10\n",
      "6/6 [==============================] - 10s 2s/step - loss: 2.2183 - accuracy: 0.7947 - val_loss: 2.1459 - val_accuracy: 0.7812\n",
      "Epoch 8/10\n",
      "6/6 [==============================] - 12s 2s/step - loss: 1.9438 - accuracy: 0.8125 - val_loss: 1.9568 - val_accuracy: 0.7812\n",
      "Epoch 9/10\n",
      "6/6 [==============================] - 11s 2s/step - loss: 1.7892 - accuracy: 0.8000 - val_loss: 1.8039 - val_accuracy: 0.7812\n",
      "Epoch 10/10\n",
      "6/6 [==============================] - 11s 2s/step - loss: 1.6573 - accuracy: 0.7895 - val_loss: 1.6707 - val_accuracy: 0.7812\n",
      "1/1 [==============================] - 1s 1s/step - loss: 1.6707 - accuracy: 0.7812\n",
      "\n",
      "\n",
      "Test accuracy: 78.12%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "# Загрузка и анализ данных\n",
    "def load_data(json_file, images_dir):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Создание словаря, который связывает каждое изображение с его меткой класса\n",
    "    image_labels = {}\n",
    "    for annotation in data['annotations']:\n",
    "        image_id = annotation['image_id']\n",
    "        category_id = annotation['category_id'] - 1  # IDs начинаются с 1\n",
    "\n",
    "        # Поиск соответствующего изображения\n",
    "        image = next(image for image in data['images'] if image['id'] == image_id)\n",
    "        image_path = os.path.join(images_dir, image['file_name'])\n",
    "        \n",
    "        # Добавление изображения и метки к словарю\n",
    "        image_labels[image_path] = category_id\n",
    "\n",
    "    return image_labels, len(data['categories'])\n",
    "\n",
    "# Подготовка данных\n",
    "train_data, num_classes = load_data('C:\\\\Users\\\\godcl\\\\jupyter\\\\tomatoOD\\\\annotations\\\\tomatOD_train.json','tomatoOD\\\\train')\n",
    "test_data, _ = load_data('C:\\\\Users\\\\godcl\\\\jupyter\\\\tomatoOD\\\\annotations\\\\tomatOD_test.json', 'tomatoOD\\\\test')\n",
    "\n",
    "# Создание экземпляра ImageDataGenerator с аугментацией\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input  # Предварительная обработка из EfficientNet\n",
    ")\n",
    "\n",
    "def train_data_generator(data, batch_size, input_size, num_classes):\n",
    "    image_paths = list(data.keys())\n",
    "    while True:\n",
    "        # Случайное перемешивание данных перед каждой эпохой\n",
    "        random.shuffle(image_paths)\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            for path in batch_paths:\n",
    "                # Загрузка изображения и преобразование его в массив\n",
    "                image = load_img(path, target_size=input_size)\n",
    "                image = img_to_array(image)\n",
    "                \n",
    "                # Аугментация изображения\n",
    "                image = train_datagen.random_transform(image)\n",
    "                image = preprocess_input(image)\n",
    "\n",
    "                label = to_categorical(data[path], num_classes=num_classes)\n",
    "                batch_images.append(image)\n",
    "                batch_labels.append(label)\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "            \n",
    "# Создание генератора данных для тестирования\n",
    "def test_data_generator(data, batch_size, input_size, num_classes):\n",
    "    image_paths = list(data.keys())\n",
    "    while True:\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            for path in batch_paths:\n",
    "                image = load_img(path, target_size=input_size)\n",
    "                image = img_to_array(image)\n",
    "                image = preprocess_input(image)\n",
    "                label = to_categorical(data[path], num_classes=num_classes)\n",
    "                batch_images.append(image)\n",
    "                batch_labels.append(label)\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "\n",
    "# Параметры\n",
    "input_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Создание модели\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=input_size + (3,))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "\n",
    "# Обучение модели\n",
    "print('Обучение', end = '\\n')\n",
    "history = model.fit(\n",
    "    train_data_generator(train_data, batch_size, input_size, num_classes),\n",
    "    steps_per_epoch=len(train_data) // batch_size,\n",
    "    epochs=10,\n",
    "    validation_data=test_data_generator(test_data, batch_size, input_size, num_classes),\n",
    "    validation_steps=len(test_data) // batch_size,\n",
    "    callbacks=[early_stopping] # добавление Early stopping\n",
    ")\n",
    "\n",
    "# Проверка точности на тестовых данных\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    test_data_generator(test_data, batch_size, input_size, num_classes),\n",
    "    steps=len(test_data) // batch_size\n",
    ")\n",
    "print('\\n')\n",
    "print(f'Test accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a5e72f9",
   "metadata": {},
   "source": [
    "#  Обучаем effnetb3(ds = 161 test, 643 train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "82982e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение\n",
      "\n",
      "\n",
      "Epoch 1/20\n",
      "20/20 [==============================] - 64s 3s/step - loss: 5.7775 - accuracy: 0.3891 - val_loss: 5.2993 - val_accuracy: 0.5750 - lr: 0.0100\n",
      "Epoch 2/20\n",
      "20/20 [==============================] - 58s 3s/step - loss: 5.1640 - accuracy: 0.5401 - val_loss: 4.8955 - val_accuracy: 0.6250 - lr: 0.0100\n",
      "Epoch 3/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 4.8259 - accuracy: 0.5532 - val_loss: 4.5175 - val_accuracy: 0.6562 - lr: 0.0100\n",
      "Epoch 4/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 4.5016 - accuracy: 0.5728 - val_loss: 4.2815 - val_accuracy: 0.5938 - lr: 0.0100\n",
      "Epoch 5/20\n",
      "20/20 [==============================] - 55s 3s/step - loss: 4.1655 - accuracy: 0.6039 - val_loss: 3.9666 - val_accuracy: 0.6250 - lr: 0.0100\n",
      "Epoch 6/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 3.9723 - accuracy: 0.5876 - val_loss: 3.7394 - val_accuracy: 0.6625 - lr: 0.0100\n",
      "Epoch 7/20\n",
      "20/20 [==============================] - 57s 3s/step - loss: 3.7430 - accuracy: 0.6072 - val_loss: 3.5231 - val_accuracy: 0.6438 - lr: 0.0100\n",
      "Epoch 8/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 3.5377 - accuracy: 0.5876 - val_loss: 3.3225 - val_accuracy: 0.6313 - lr: 0.0100\n",
      "Epoch 9/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 3.2703 - accuracy: 0.6219 - val_loss: 3.1654 - val_accuracy: 0.6375 - lr: 0.0100\n",
      "Epoch 10/20\n",
      "20/20 [==============================] - 55s 3s/step - loss: 3.1483 - accuracy: 0.5974 - val_loss: 2.9786 - val_accuracy: 0.6313 - lr: 0.0100\n",
      "Epoch 11/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.9556 - accuracy: 0.6448 - val_loss: 2.8206 - val_accuracy: 0.6750 - lr: 0.0100\n",
      "Epoch 12/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.7551 - accuracy: 0.6547 - val_loss: 2.6872 - val_accuracy: 0.6625 - lr: 0.0100\n",
      "Epoch 13/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.6095 - accuracy: 0.6448 - val_loss: 2.5919 - val_accuracy: 0.6687 - lr: 0.0100\n",
      "Epoch 14/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.5366 - accuracy: 0.6350 - val_loss: 2.4781 - val_accuracy: 0.6250 - lr: 0.0100\n",
      "Epoch 15/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.3697 - accuracy: 0.6579 - val_loss: 2.3501 - val_accuracy: 0.6625 - lr: 0.0100\n",
      "Epoch 16/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.3106 - accuracy: 0.6187 - val_loss: 2.2378 - val_accuracy: 0.6687 - lr: 0.0100\n",
      "Epoch 17/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.1594 - accuracy: 0.6530 - val_loss: 2.1437 - val_accuracy: 0.6375 - lr: 0.0100\n",
      "Epoch 18/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.1138 - accuracy: 0.6203 - val_loss: 2.1206 - val_accuracy: 0.6562 - lr: 0.0100\n",
      "Epoch 19/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 2.0499 - accuracy: 0.6301 - val_loss: 1.9955 - val_accuracy: 0.6313 - lr: 0.0100\n",
      "Epoch 20/20\n",
      "20/20 [==============================] - 56s 3s/step - loss: 1.9262 - accuracy: 0.6498 - val_loss: 1.9033 - val_accuracy: 0.6562 - lr: 0.0100\n",
      "5/5 [==============================] - 10s 2s/step - loss: 1.9033 - accuracy: 0.6562\n",
      "\n",
      "\n",
      "Test accuracy: 65.62%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "# Загрузка и анализ данных\n",
    "def load_data(json_file, images_dir):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Создание словаря, который связывает каждое изображение с его меткой класса\n",
    "    image_labels = {}\n",
    "    for annotation in data['annotations']:\n",
    "        image_id = annotation['image_id']\n",
    "        category_id = annotation['category_id'] - 1  # IDs начинаются с 1\n",
    "\n",
    "        # Поиск соответствующего изображения\n",
    "        image = next(image for image in data['images'] if image['id'] == image_id)\n",
    "        image_path = os.path.join(images_dir, image['file_name'])\n",
    "        \n",
    "        # Добавление изображения и метки к словарю\n",
    "        image_labels[image_path] = category_id\n",
    "\n",
    "    return image_labels, len(data['categories'])\n",
    "\n",
    "# Подготовка данных\n",
    "train_data, num_classes = load_data('C:\\\\Users\\\\godcl\\\\jupyter\\\\laboro_tomato\\\\annotations\\\\train.json','laboro_tomato\\\\train')\n",
    "test_data, _ = load_data('C:\\\\Users\\\\godcl\\\\jupyter\\\\laboro_tomato\\\\annotations\\\\test.json', 'laboro_tomato\\\\test')\n",
    "\n",
    "# Создание экземпляра ImageDataGenerator с аугментацией\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input  # Предварительная обработка из EfficientNet\n",
    ")\n",
    "\n",
    "def train_data_generator(data, batch_size, input_size, num_classes):\n",
    "    image_paths = list(data.keys())\n",
    "    while True:\n",
    "        # Случайное перемешивание данных перед каждой эпохой\n",
    "        random.shuffle(image_paths)\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            for path in batch_paths:\n",
    "                # Загрузка изображения и преобразование его в массив\n",
    "                image = load_img(path, target_size=input_size)\n",
    "                image = img_to_array(image)\n",
    "                \n",
    "                # Аугментация изображения\n",
    "                image = train_datagen.random_transform(image)\n",
    "                image = preprocess_input(image)\n",
    "\n",
    "                label = to_categorical(data[path], num_classes=num_classes)\n",
    "                batch_images.append(image)\n",
    "                batch_labels.append(label)\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "            \n",
    "# Создание генератора данных для тестирования\n",
    "def test_data_generator(data, batch_size, input_size, num_classes):\n",
    "    image_paths = list(data.keys())\n",
    "    while True:\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            for path in batch_paths:\n",
    "                image = load_img(path, target_size=input_size)\n",
    "                image = img_to_array(image)\n",
    "                image = preprocess_input(image)\n",
    "                label = to_categorical(data[path], num_classes=num_classes)\n",
    "                batch_images.append(image)\n",
    "                batch_labels.append(label)\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "          \n",
    "            \n",
    "# Параметры\n",
    "input_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Создание модели\n",
    "base_model = EfficientNetB0(include_top=False, weights='imagenet', input_shape=input_size + (3,))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.01,\n",
    "    decay_steps=10000,\n",
    "    decay_rate=0.9)\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, nesterov=True)\n",
    "\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [early_stopping, lrate]  \n",
    "# Обучение модели\n",
    "print('Обучение', end = '\\n')\n",
    "print('\\n')\n",
    "history = model.fit(\n",
    "    train_data_generator(train_data, batch_size, input_size, num_classes),\n",
    "    steps_per_epoch=len(train_data) // batch_size,\n",
    "    epochs=20,\n",
    "    validation_data=test_data_generator(test_data, batch_size, input_size, num_classes),\n",
    "    validation_steps=len(test_data) // batch_size,\n",
    "    callbacks=callbacks_list \n",
    ")\n",
    "\n",
    "# Проверка точности на тестовых данных\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    test_data_generator(test_data, batch_size, input_size, num_classes),\n",
    "    steps=len(test_data) // batch_size\n",
    ")\n",
    "print('\\n')\n",
    "print(f'Test accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96e1d9e1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Layer count mismatch when loading weights from file. Model expected 210 layers, found 211 saved layers.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m model \u001b[38;5;241m=\u001b[39m Model(inputs\u001b[38;5;241m=\u001b[39mbase_model\u001b[38;5;241m.\u001b[39minput, outputs\u001b[38;5;241m=\u001b[39mx)\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# Загрузка сохраненных весов\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_weights\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC://Users//godcl//jupyter//effnetb3-laboro(71.88)_weights.h5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m predictions \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(image_array)\n\u001b[0;32m     38\u001b[0m class_labels \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb_fully_ripened\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb_half_ripened\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb_green\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml_fully_ripened\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml_half_ripened\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124ml_green\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\saving\\legacy\\hdf5_format.py:808\u001b[0m, in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, model)\u001b[0m\n\u001b[0;32m    806\u001b[0m layer_names \u001b[38;5;241m=\u001b[39m filtered_layer_names\n\u001b[0;32m    807\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(layer_names) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(filtered_layers):\n\u001b[1;32m--> 808\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    809\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLayer count mismatch when loading weights from file. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    810\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel expected \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(filtered_layers)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m layers, found \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    811\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(layer_names)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m saved layers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    812\u001b[0m     )\n\u001b[0;32m    814\u001b[0m \u001b[38;5;66;03m# We batch weight value assignments in a single backend call\u001b[39;00m\n\u001b[0;32m    815\u001b[0m \u001b[38;5;66;03m# which provides a speedup in TensorFlow.\u001b[39;00m\n\u001b[0;32m    816\u001b[0m weight_value_tuples \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[1;31mValueError\u001b[0m: Layer count mismatch when loading weights from file. Model expected 210 layers, found 211 saved layers."
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "image_path = r'C://Users//Screenshot_1.png'\n",
    "image = Image.open(image_path)\n",
    "image = image.resize((300, 300))  # Размеры EfficientNetB3\n",
    "\n",
    "# Преобразование изображения в массив numpy\n",
    "image_array = img_to_array(image)\n",
    "\n",
    "# Нормализация пикселей с помощью функции preprocess_input из EfficientNet\n",
    "image_array = preprocess_input(image_array)\n",
    "\n",
    "# Добавляем измерение размера пакета (batch size) для классификации одного изображения\n",
    "image_array = np.expand_dims(image_array, axis=0)\n",
    "\n",
    "# Создание модели EfficientNetB3 без последнего слоя для классификации\n",
    "base_model = EfficientNetB3(weights='imagenet', include_top=False, input_shape=(300, 300, 3))\n",
    "\n",
    "# Добавление своего последнего слоя для классификации (6 классов)\n",
    "x = base_model.output\n",
    "x = Dense(6, activation='softmax')(x)\n",
    "\n",
    "# Сборка модели\n",
    "model = Model(inputs=base_model.input, outputs=x)\n",
    "\n",
    "# Загрузка сохраненных весов\n",
    "model.load_weights('C://Users//godcl//jupyter//effnetb3-laboro(71.88)_weights.h5')\n",
    "\n",
    "predictions = model.predict(image_array)\n",
    "\n",
    "class_labels = ['b_fully_ripened', 'b_half_ripened', 'b_green', 'l_fully_ripened', 'l_half_ripened', 'l_green']\n",
    "\n",
    "# Находим индекс класса с максимальной вероятностью\n",
    "predicted_class_index = np.argmax(predictions)\n",
    "\n",
    "# Получаем название класса\n",
    "predicted_class_label = class_labels[predicted_class_index]\n",
    "\n",
    "print(\"Предсказанный класс:\", predicted_class_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd37ee5",
   "metadata": {},
   "source": [
    "# Бесполезный трай в 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "704f200e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Обучение\n",
      "\n",
      "\n",
      "Epoch 1/50\n",
      "20/20 [==============================] - 125s 6s/step - loss: 5.8852 - accuracy: 0.4000 - val_loss: 5.3358 - val_accuracy: 0.6000 - lr: 0.0100\n",
      "Epoch 2/50\n",
      "20/20 [==============================] - 106s 6s/step - loss: 5.2762 - accuracy: 0.5401 - val_loss: 4.9371 - val_accuracy: 0.6062 - lr: 0.0100\n",
      "Epoch 3/50\n",
      "20/20 [==============================] - 130s 7s/step - loss: 4.8362 - accuracy: 0.6088 - val_loss: 4.6150 - val_accuracy: 0.6500 - lr: 0.0100\n",
      "Epoch 4/50\n",
      "20/20 [==============================] - 101s 5s/step - loss: 4.5022 - accuracy: 0.5908 - val_loss: 4.3534 - val_accuracy: 0.6250 - lr: 0.0100\n",
      "Epoch 5/50\n",
      "20/20 [==============================] - 97s 5s/step - loss: 4.1828 - accuracy: 0.6121 - val_loss: 4.0109 - val_accuracy: 0.6500 - lr: 0.0100\n",
      "Epoch 6/50\n",
      "20/20 [==============================] - 93s 5s/step - loss: 3.8046 - accuracy: 0.6841 - val_loss: 3.7684 - val_accuracy: 0.6750 - lr: 0.0100\n",
      "Epoch 7/50\n",
      "20/20 [==============================] - 88s 5s/step - loss: 3.6633 - accuracy: 0.6498 - val_loss: 3.5415 - val_accuracy: 0.6562 - lr: 0.0100\n",
      "Epoch 8/50\n",
      "20/20 [==============================] - 91s 5s/step - loss: 3.3652 - accuracy: 0.6759 - val_loss: 3.3647 - val_accuracy: 0.6500 - lr: 0.0100\n",
      "Epoch 9/50\n",
      "20/20 [==============================] - 99s 5s/step - loss: 3.1985 - accuracy: 0.6825 - val_loss: 3.1500 - val_accuracy: 0.6875 - lr: 0.0100\n",
      "Epoch 10/50\n",
      "20/20 [==============================] - 102s 5s/step - loss: 2.9257 - accuracy: 0.7283 - val_loss: 3.0760 - val_accuracy: 0.6625 - lr: 0.0100\n",
      "Epoch 11/50\n",
      "20/20 [==============================] - 103s 5s/step - loss: 2.7814 - accuracy: 0.6841 - val_loss: 2.8204 - val_accuracy: 0.7000 - lr: 0.0100\n",
      "Epoch 12/50\n",
      "20/20 [==============================] - 93s 5s/step - loss: 2.6379 - accuracy: 0.6956 - val_loss: 2.7213 - val_accuracy: 0.6438 - lr: 0.0100\n",
      "Epoch 13/50\n",
      "20/20 [==============================] - 101s 5s/step - loss: 2.4680 - accuracy: 0.7365 - val_loss: 2.5805 - val_accuracy: 0.6687 - lr: 0.0100\n",
      "Epoch 14/50\n",
      "20/20 [==============================] - 97s 5s/step - loss: 2.3138 - accuracy: 0.7283 - val_loss: 2.4789 - val_accuracy: 0.6812 - lr: 0.0100\n",
      "Epoch 15/50\n",
      "20/20 [==============================] - 99s 5s/step - loss: 2.1562 - accuracy: 0.7578 - val_loss: 2.4117 - val_accuracy: 0.7125 - lr: 0.0100\n",
      "Epoch 16/50\n",
      "20/20 [==============================] - 100s 5s/step - loss: 2.0661 - accuracy: 0.7692 - val_loss: 2.2786 - val_accuracy: 0.6938 - lr: 0.0100\n",
      "Epoch 17/50\n",
      "20/20 [==============================] - 108s 6s/step - loss: 1.9432 - accuracy: 0.7447 - val_loss: 2.1401 - val_accuracy: 0.7250 - lr: 0.0100\n",
      "Epoch 18/50\n",
      "20/20 [==============================] - 102s 5s/step - loss: 1.8339 - accuracy: 0.7627 - val_loss: 2.1224 - val_accuracy: 0.6750 - lr: 0.0100\n",
      "Epoch 19/50\n",
      "20/20 [==============================] - 102s 5s/step - loss: 1.7639 - accuracy: 0.7709 - val_loss: 2.1179 - val_accuracy: 0.6438 - lr: 0.0100\n",
      "Epoch 20/50\n",
      "20/20 [==============================] - 101s 5s/step - loss: 1.6368 - accuracy: 0.8134 - val_loss: 1.9833 - val_accuracy: 0.6562 - lr: 0.0100\n",
      "Epoch 21/50\n",
      "20/20 [==============================] - 97s 5s/step - loss: 1.5895 - accuracy: 0.7741 - val_loss: 1.9545 - val_accuracy: 0.6313 - lr: 0.0100\n",
      "Epoch 22/50\n",
      "20/20 [==============================] - 106s 5s/step - loss: 1.4618 - accuracy: 0.8062 - val_loss: 1.9116 - val_accuracy: 0.6250 - lr: 0.0100\n",
      "Epoch 23/50\n",
      "20/20 [==============================] - 102s 5s/step - loss: 1.4274 - accuracy: 0.7938 - val_loss: 1.8647 - val_accuracy: 0.6375 - lr: 0.0100\n",
      "Epoch 24/50\n",
      "20/20 [==============================] - 98s 5s/step - loss: 1.3456 - accuracy: 0.8085 - val_loss: 1.7730 - val_accuracy: 0.6500 - lr: 0.0099\n",
      "Epoch 25/50\n",
      "20/20 [==============================] - 96s 5s/step - loss: 1.2780 - accuracy: 0.8069 - val_loss: 1.7776 - val_accuracy: 0.6250 - lr: 0.0099\n",
      "Epoch 26/50\n",
      "20/20 [==============================] - 106s 5s/step - loss: 1.2077 - accuracy: 0.8216 - val_loss: 1.6582 - val_accuracy: 0.6500 - lr: 0.0099\n",
      "Epoch 27/50\n",
      "20/20 [==============================] - 104s 5s/step - loss: 1.1834 - accuracy: 0.8232 - val_loss: 1.6393 - val_accuracy: 0.6562 - lr: 0.0099\n",
      "Epoch 28/50\n",
      "20/20 [==============================] - 91s 5s/step - loss: 1.1351 - accuracy: 0.8134 - val_loss: 1.6673 - val_accuracy: 0.6125 - lr: 0.0099\n",
      "Epoch 29/50\n",
      "20/20 [==============================] - 96s 5s/step - loss: 1.0885 - accuracy: 0.8282 - val_loss: 1.6772 - val_accuracy: 0.6812 - lr: 0.0099\n",
      "Epoch 30/50\n",
      "20/20 [==============================] - 94s 5s/step - loss: 0.9709 - accuracy: 0.8674 - val_loss: 1.5618 - val_accuracy: 0.6438 - lr: 0.0099\n",
      "Epoch 31/50\n",
      "20/20 [==============================] - 84s 4s/step - loss: 0.9857 - accuracy: 0.8543 - val_loss: 1.5599 - val_accuracy: 0.6750 - lr: 0.0099\n",
      "Epoch 32/50\n",
      "20/20 [==============================] - 104s 5s/step - loss: 0.9628 - accuracy: 0.8543 - val_loss: 1.5641 - val_accuracy: 0.6875 - lr: 0.0099\n",
      "Epoch 33/50\n",
      "20/20 [==============================] - 95s 5s/step - loss: 0.9007 - accuracy: 0.8445 - val_loss: 1.5394 - val_accuracy: 0.6938 - lr: 0.0099\n",
      "Epoch 34/50\n",
      "20/20 [==============================] - 78s 4s/step - loss: 0.9194 - accuracy: 0.8380 - val_loss: 1.5075 - val_accuracy: 0.6750 - lr: 0.0099\n",
      "Epoch 35/50\n",
      "20/20 [==============================] - 77s 4s/step - loss: 0.8060 - accuracy: 0.8773 - val_loss: 1.5028 - val_accuracy: 0.6687 - lr: 0.0099\n",
      "Epoch 36/50\n",
      "20/20 [==============================] - 77s 4s/step - loss: 0.8145 - accuracy: 0.8625 - val_loss: 1.5198 - val_accuracy: 0.6062 - lr: 0.0099\n",
      "Epoch 37/50\n",
      "20/20 [==============================] - 77s 4s/step - loss: 0.7993 - accuracy: 0.8642 - val_loss: 1.4495 - val_accuracy: 0.6875 - lr: 0.0099\n",
      "Epoch 38/50\n",
      "20/20 [==============================] - 77s 4s/step - loss: 0.7754 - accuracy: 0.8674 - val_loss: 1.4868 - val_accuracy: 0.6812 - lr: 0.0099\n",
      "Epoch 39/50\n",
      "20/20 [==============================] - 77s 4s/step - loss: 0.7627 - accuracy: 0.8642 - val_loss: 1.5011 - val_accuracy: 0.6562 - lr: 0.0099\n",
      "Epoch 40/50\n",
      "20/20 [==============================] - 76s 4s/step - loss: 0.6683 - accuracy: 0.9051 - val_loss: 1.5273 - val_accuracy: 0.6187 - lr: 0.0099\n",
      "5/5 [==============================] - 13s 3s/step - loss: 1.5273 - accuracy: 0.6187\n",
      "\n",
      "\n",
      "Test accuracy: 61.87%\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "# Загрузка и анализ данных\n",
    "def load_data(json_file, images_dir):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Создание словаря, который связывает каждое изображение с его меткой класса\n",
    "    image_labels = {}\n",
    "    for annotation in data['annotations']:\n",
    "        image_id = annotation['image_id']\n",
    "        category_id = annotation['category_id'] - 1  # IDs начинаются с 1\n",
    "\n",
    "        # Поиск соответствующего изображения\n",
    "        image = next(image for image in data['images'] if image['id'] == image_id)\n",
    "        image_path = os.path.join(images_dir, image['file_name'])\n",
    "        \n",
    "        # Добавление изображения и метки к словарю\n",
    "        image_labels[image_path] = category_id\n",
    "\n",
    "    return image_labels, len(data['categories'])\n",
    "\n",
    "# Подготовка данных\n",
    "train_data, num_classes = load_data('C:\\\\Users\\\\godcl\\\\jupyter\\\\laboro_tomato\\\\annotations\\\\train.json','laboro_tomato\\\\train')\n",
    "test_data, _ = load_data('C:\\\\Users\\\\godcl\\\\jupyter\\\\laboro_tomato\\\\annotations\\\\test.json', 'laboro_tomato\\\\test')\n",
    "\n",
    "# Создание экземпляра ImageDataGenerator с аугментацией\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input  # Предварительная обработка из EfficientNet\n",
    ")\n",
    "\n",
    "def train_data_generator(data, batch_size, input_size, num_classes):\n",
    "    image_paths = list(data.keys())\n",
    "    while True:\n",
    "        # Случайное перемешивание данных перед каждой эпохой\n",
    "        random.shuffle(image_paths)\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            for path in batch_paths:\n",
    "                # Загрузка изображения и преобразование его в массив\n",
    "                image = load_img(path, target_size=input_size)\n",
    "                image = img_to_array(image)\n",
    "                \n",
    "                # Аугментация изображения\n",
    "                image = train_datagen.random_transform(image)\n",
    "                image = preprocess_input(image)\n",
    "\n",
    "                label = to_categorical(data[path], num_classes=num_classes)\n",
    "                batch_images.append(image)\n",
    "                batch_labels.append(label)\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "            \n",
    "# Создание генератора данных для тестирования\n",
    "def test_data_generator(data, batch_size, input_size, num_classes):\n",
    "    image_paths = list(data.keys())\n",
    "    while True:\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            for path in batch_paths:\n",
    "                image = load_img(path, target_size=input_size)\n",
    "                image = img_to_array(image)\n",
    "                image = preprocess_input(image)\n",
    "                label = to_categorical(data[path], num_classes=num_classes)\n",
    "                batch_images.append(image)\n",
    "                batch_labels.append(label)\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "          \n",
    "            \n",
    "# Параметры\n",
    "input_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Создание модели\n",
    "base_model = EfficientNetB3(include_top=False, weights='imagenet', input_shape=input_size + (3,))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "     initial_learning_rate=0.01,\n",
    "     decay_steps=10000,\n",
    "     decay_rate=0.9)\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, nesterov=True)\n",
    "\n",
    "for layer in model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [early_stopping, lrate]  \n",
    "# Обучение модели\n",
    "print('Обучение', end = '\\n')\n",
    "print('\\n')\n",
    "history = model.fit(\n",
    "    train_data_generator(train_data, batch_size, input_size, num_classes),\n",
    "    steps_per_epoch=len(train_data) // batch_size,\n",
    "    epochs=20,\n",
    "    validation_data=test_data_generator(test_data, batch_size, input_size, num_classes),\n",
    "    validation_steps=len(test_data) // batch_size,\n",
    "    callbacks=callbacks_list \n",
    ")\n",
    "\n",
    "# Проверка точности на тестовых данных\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    test_data_generator(test_data, batch_size, input_size, num_classes),\n",
    "    steps=len(test_data) // batch_size\n",
    ")\n",
    "print('\\n')\n",
    "print(f'Test accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e063db1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import math\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from PIL import Image\n",
    "from tensorflow.keras.preprocessing.image import load_img, img_to_array\n",
    "from tensorflow.keras.applications.efficientnet import preprocess_input\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.applications import EfficientNetB3\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Flatten, Dropout\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils import Sequence\n",
    "from tensorflow.keras.callbacks import EarlyStopping, LearningRateScheduler\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "\n",
    "# Загрузка и анализ данных\n",
    "def load_data(json_file, images_dir):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "    # Создание словаря, который связывает каждое изображение с его меткой класса\n",
    "    image_labels = {}\n",
    "    for annotation in data['annotations']:\n",
    "        image_id = annotation['image_id']\n",
    "        category_id = annotation['category_id'] - 1  # IDs начинаются с 1\n",
    "\n",
    "        # Поиск соответствующего изображения\n",
    "        image = next(image for image in data['images'] if image['id'] == image_id)\n",
    "        image_path = os.path.join(images_dir, image['file_name'])\n",
    "        \n",
    "        # Добавление изображения и метки к словарю\n",
    "        image_labels[image_path] = category_id\n",
    "\n",
    "    return image_labels, len(data['categories'])\n",
    "\n",
    "# Подготовка данных\n",
    "train_data, num_classes = load_data('C:\\\\Users\\\\godcl\\\\jupyter\\\\laboro_tomato\\\\annotations\\\\train.json','laboro_tomato\\\\train')\n",
    "test_data, _ = load_data('C:\\\\Users\\\\godcl\\\\jupyter\\\\laboro_tomato\\\\annotations\\\\test.json', 'laboro_tomato\\\\test')\n",
    "\n",
    "# Создание экземпляра ImageDataGenerator с аугментацией\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input  # Предварительная обработка из EfficientNet\n",
    ")\n",
    "\n",
    "def train_data_generator(data, batch_size, input_size, num_classes):\n",
    "    image_paths = list(data.keys())\n",
    "    while True:\n",
    "        # Случайное перемешивание данных перед каждой эпохой\n",
    "        random.shuffle(image_paths)\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            for path in batch_paths:\n",
    "                # Загрузка изображения и преобразование его в массив\n",
    "                image = load_img(path, target_size=input_size)\n",
    "                image = img_to_array(image)\n",
    "                \n",
    "                # Аугментация изображения\n",
    "                image = train_datagen.random_transform(image)\n",
    "                image = preprocess_input(image)\n",
    "\n",
    "                label = to_categorical(data[path], num_classes=num_classes)\n",
    "                batch_images.append(image)\n",
    "                batch_labels.append(label)\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "            \n",
    "# Создание генератора данных для тестирования\n",
    "def test_data_generator(data, batch_size, input_size, num_classes):\n",
    "    image_paths = list(data.keys())\n",
    "    while True:\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            for path in batch_paths:\n",
    "                image = load_img(path, target_size=input_size)\n",
    "                image = img_to_array(image)\n",
    "                image = preprocess_input(image)\n",
    "                label = to_categorical(data[path], num_classes=num_classes)\n",
    "                batch_images.append(image)\n",
    "                batch_labels.append(label)\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "          \n",
    "            \n",
    "# Параметры\n",
    "input_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Создание модели\n",
    "base_model = EfficientNetB3(include_top=False, weights='imagenet', input_shape=input_size + (3,))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "     initial_learning_rate=0.01,\n",
    "     decay_steps=10000,\n",
    "     decay_rate=0.9)\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, nesterov=True)\n",
    "\n",
    "for layer in model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [early_stopping, lrate]  \n",
    "# Обучение модели\n",
    "print('Обучение', end = '\\n')\n",
    "print('\\n')\n",
    "history = model.fit(\n",
    "    train_data_generator(train_data, batch_size, input_size, num_classes),\n",
    "    steps_per_epoch=len(train_data) // batch_size,\n",
    "    epochs=20,\n",
    "    validation_data=test_data_generator(test_data, batch_size, input_size, num_classes),\n",
    "    validation_steps=len(test_data) // batch_size,\n",
    "    callbacks=callbacks_list \n",
    ")\n",
    "\n",
    "# Проверка точности на тестовых данных\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    test_data_generator(test_data, batch_size, input_size, num_classes),\n",
    "    steps=len(test_data) // batch_size\n",
    ")\n",
    "print('\\n')\n",
    "print(f'Test accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03ce96d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "model = load_model('effnetb3-laboro(71.88)_weights.h5')\n",
    "predictions = model.predict(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "43db225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('effnetb3-laboro(50epoch)_weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16ec58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "    Размораживаем 20 слоев и дообучаем\n",
    "for layer in model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=0.0001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(train_generator, epochs=8, callbacks=[early_stopping])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ce37a4",
   "metadata": {},
   "source": [
    "# PASCAL VOC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "35537942",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'os' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 39\u001b[0m\n\u001b[0;32m     35\u001b[0m         image_labels[image_path] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(class_id)\n\u001b[0;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m image_labels, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mset\u001b[39m(image_labels\u001b[38;5;241m.\u001b[39mvalues()))\n\u001b[1;32m---> 39\u001b[0m train_data, num_classes \u001b[38;5;241m=\u001b[39m \u001b[43mload_data\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mgodcl\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mРабочий стол\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mTomatoPlantfactoryDataset\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mimages\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mgodcl\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mРабочий стол\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mTomatoPlantfactoryDataset\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mannotations\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m                                    \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mUsers\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mgodcl\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mРабочий стол\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mTomatoPlantfactoryDataset\u001b[39;49m\u001b[38;5;130;43;01m\\\\\u001b[39;49;00m\u001b[38;5;124;43mlabels\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m test_data, _ \u001b[38;5;241m=\u001b[39m load_data(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mgodcl\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mРабочий стол\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mTomatoPlantfactoryDataset\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mimages\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     44\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mgodcl\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mРабочий стол\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mTomatoPlantfactoryDataset\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mannotations\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     45\u001b[0m                          \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mUsers\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mgodcl\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mРабочий стол\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mTomatoPlantfactoryDataset\u001b[39m\u001b[38;5;130;01m\\\\\u001b[39;00m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Создание экземпляра ImageDataGenerator с аугментацией\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m, in \u001b[0;36mload_data\u001b[1;34m(images_dir, annotations_dir, labels_dir)\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_data\u001b[39m(images_dir, annotations_dir, labels_dir):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Получение списка всех изображений и аннотаций\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m     images \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241m.\u001b[39mlistdir(images_dir)\n\u001b[0;32m      6\u001b[0m     annotations \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(annotations_dir)\n\u001b[0;32m      7\u001b[0m     labels \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mlistdir(labels_dir)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'os' is not defined"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def load_data(images_dir, annotations_dir, labels_dir):\n",
    "    # Получение списка всех изображений и аннотаций\n",
    "    images = os.listdir(images_dir)\n",
    "    annotations = os.listdir(annotations_dir)\n",
    "    labels = os.listdir(labels_dir)\n",
    "\n",
    "    # Создание словаря, который связывает каждое изображение с его меткой класса\n",
    "    image_labels = {}\n",
    "    for image in images:\n",
    "        image_name, _ = os.path.splitext(image)\n",
    "        annotation_file = os.path.join(annotations_dir, f\"{image_name}.xml\")\n",
    "        label_file = os.path.join(labels_dir, f\"{image_name}.txt\")\n",
    "\n",
    "        # Парсинг XML-файла\n",
    "        tree = ET.parse(annotation_file)\n",
    "        root = tree.getroot()\n",
    "\n",
    "        # Получение метки класса\n",
    "        for member in root.findall('object'):\n",
    "            label = member[0].text\n",
    "            if label == 'tomato':\n",
    "                # Метка класса для помидора\n",
    "                class_id = 0\n",
    "\n",
    "        # Парсинг TXT-файла\n",
    "        with open(label_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "            for line in lines:\n",
    "                class_id, _, _, _, _ = map(float, line.split())\n",
    "\n",
    "        # Добавление изображения и метки к словарю\n",
    "        image_path = os.path.join(images_dir, image)\n",
    "        image_labels[image_path] = int(class_id)\n",
    "\n",
    "    return image_labels, len(set(image_labels.values()))\n",
    "\n",
    "train_data, num_classes = load_data('C:\\\\Users\\\\godcl\\\\Рабочий стол\\\\TomatoPlantfactoryDataset\\\\images',\n",
    "                                    'C:\\\\Users\\\\godcl\\\\Рабочий стол\\\\TomatoPlantfactoryDataset\\\\annotations',\n",
    "                                    'C:\\\\Users\\\\godcl\\\\Рабочий стол\\\\TomatoPlantfactoryDataset\\\\labels')\n",
    "\n",
    "test_data, _ = load_data('C:\\\\Users\\\\godcl\\\\Рабочий стол\\\\TomatoPlantfactoryDataset\\\\images',\n",
    "                         'C:\\\\Users\\\\godcl\\\\Рабочий стол\\\\TomatoPlantfactoryDataset\\\\annotations',\n",
    "                         'C:\\\\Users\\\\godcl\\\\Рабочий стол\\\\TomatoPlantfactoryDataset\\\\labels')\n",
    "\n",
    "# Создание экземпляра ImageDataGenerator с аугментацией\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    preprocessing_function=preprocess_input  # Предварительная обработка из EfficientNet\n",
    ")\n",
    "\n",
    "def train_data_generator(data, batch_size, input_size, num_classes):\n",
    "    image_paths = list(data.keys())\n",
    "    while True:\n",
    "        # Случайное перемешивание данных перед каждой эпохой\n",
    "        random.shuffle(image_paths)\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            for path in batch_paths:\n",
    "                # Загрузка изображения и преобразование его в массив\n",
    "                image = load_img(path, target_size=input_size)\n",
    "                image = img_to_array(image)\n",
    "                \n",
    "                # Аугментация изображения\n",
    "                image = train_datagen.random_transform(image)\n",
    "                image = preprocess_input(image)\n",
    "\n",
    "                label = to_categorical(data[path], num_classes=num_classes)\n",
    "                batch_images.append(image)\n",
    "                batch_labels.append(label)\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "            \n",
    "# Создание генератора данных для тестирования\n",
    "def test_data_generator(data, batch_size, input_size, num_classes):\n",
    "    image_paths = list(data.keys())\n",
    "    while True:\n",
    "        for i in range(0, len(image_paths), batch_size):\n",
    "            batch_paths = image_paths[i:i+batch_size]\n",
    "            batch_images = []\n",
    "            batch_labels = []\n",
    "            for path in batch_paths:\n",
    "                image = load_img(path, target_size=input_size)\n",
    "                image = img_to_array(image)\n",
    "                image = preprocess_input(image)\n",
    "                label = to_categorical(data[path], num_classes=num_classes)\n",
    "                batch_images.append(image)\n",
    "                batch_labels.append(label)\n",
    "            yield np.array(batch_images), np.array(batch_labels)\n",
    "\n",
    "\n",
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.01\n",
    "    drop = 0.5\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1+epoch)/epochs_drop))\n",
    "    return lrate\n",
    "\n",
    "          \n",
    "            \n",
    "# Параметры\n",
    "input_size = (224, 224)\n",
    "batch_size = 32\n",
    "\n",
    "# Создание модели\n",
    "base_model = EfficientNetB3(include_top=False, weights='imagenet', input_shape=input_size + (3,))\n",
    "\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "    \n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Flatten()(x)\n",
    "x = Dense(256, activation='relu',kernel_regularizer=regularizers.l2(0.01))(x)\n",
    "x = Dropout(0.5)(x)\n",
    "predictions = Dense(num_classes, activation='softmax')(x)\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "# Compile\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "     initial_learning_rate=0.01,\n",
    "     decay_steps=10000,\n",
    "     decay_rate=0.9)\n",
    "sgd = tf.keras.optimizers.SGD(learning_rate=lr_schedule, momentum=0.9, nesterov=True)\n",
    "\n",
    "for layer in model.layers[-20:]:\n",
    "    layer.trainable = True\n",
    "    \n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Early stopping\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=3)\n",
    "lrate = LearningRateScheduler(step_decay)\n",
    "callbacks_list = [early_stopping, lrate]  \n",
    "# Обучение модели\n",
    "print('Обучение', end = '\\n')\n",
    "print('\\n')\n",
    "history = model.fit(\n",
    "    train_data_generator(train_data, batch_size, input_size, num_classes),\n",
    "    steps_per_epoch=len(train_data) // batch_size,\n",
    "    epochs=20,\n",
    "    validation_data=test_data_generator(test_data, batch_size, input_size, num_classes),\n",
    "    validation_steps=len(test_data) // batch_size,\n",
    "    callbacks=callbacks_list \n",
    ")\n",
    "\n",
    "# Проверка точности на тестовых данных\n",
    "test_loss, test_accuracy = model.evaluate(\n",
    "    test_data_generator(test_data, batch_size, input_size, num_classes),\n",
    "    steps=len(test_data) // batch_size\n",
    ")\n",
    "print('\\n')\n",
    "print(f'Test accuracy: {test_accuracy * 100:.2f}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee87c1aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
